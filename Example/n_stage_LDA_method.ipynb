{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "n-stage LDA method.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vci2GXmtRmiA"
      },
      "source": [
        "Requirement libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ufOLwKRDRlwR"
      },
      "source": [
        "!pip install gensim\n",
        "!pip install PorterStemmer \n",
        "!pip install stop_words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u7t34B9hUYC7"
      },
      "source": [
        "from nltk.stem.porter import PorterStemmer\n",
        "from stop_words import get_stop_words\n",
        "import gensim\n",
        "import string\n",
        "from gensim import corpora, models\n",
        "from gensim.models import LdaModel\n",
        "from gensim.models import CoherenceModel\n",
        "import time"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "--K2QsJXNr2N"
      },
      "source": [
        "**Read your file**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p5YjCxzSNFPL"
      },
      "source": [
        "def readFile(fileName):\n",
        "    file = open(fileName, encoding='utf-8')\n",
        "    lines = file.readlines()\n",
        "    file.close()\n",
        "\n",
        "    file_ = []\n",
        "    for line in lines:\n",
        "        file_.append(str(line).rstrip(\"\\n\"))\n",
        "\n",
        "    return file_"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6x1OBB3wVxsH"
      },
      "source": [
        "Example file is readed:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YuX7phMtSW7I"
      },
      "source": [
        "doc_set = readFile(\"uci-news3C.txt\")"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xzTEE2l3OC94"
      },
      "source": [
        "**Pre-processing and Create corpus&dictionary for LDA**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nBH-kIspOJN3"
      },
      "source": [
        "sentence_tokens = []\n",
        "model_name = 'EnglishNews'\n",
        "def englishSentence(doc):\n",
        "    p_stemmer = PorterStemmer() #stemming library for English\n",
        "    en_stop = get_stop_words('english') #english stopwords library\n",
        "    totalWords = []\n",
        "    number = [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\"]\n",
        "    for i in doc:\n",
        "        map = i.maketrans('', '', string.punctuation) #Applying punctuation process\n",
        "        out = i.translate(map)\n",
        "        tokens = []\n",
        "        not_verb = []\n",
        "        for word in str(out).split(\" \"):\n",
        "            word = str(word).lower().strip()\n",
        "            tokens.append(word)        \n",
        "        stemmed = [ p_stemmer.stem(i) for i in tokens ] #Stemming process for words\n",
        "        stemmed_tokens = [ i for i in stemmed if not i in en_stop]  #Removing stopwords\n",
        "\n",
        "        delete = []    \n",
        "        dataControl = True\n",
        "        for root in stemmed_tokens: #Deleting numbers and single letter words from word roots\n",
        "            if len(root) <= 1:\n",
        "                delete.append(root)\n",
        "            else:\n",
        "                for s in number:\n",
        "                    if root[0].find(s) != -1:\n",
        "                        delete.append(root)\n",
        "                        dataControl = False\n",
        "                        break\n",
        "                if dataControl:\n",
        "                    totalWords.append(root)\n",
        "        for d in delete:\n",
        "            stemmed_tokens.remove(d)\n",
        "                    \n",
        "        sentence_tokens.append(stemmed_tokens)   #Create sentence tokens for gensim.LDA\n",
        "    \n",
        "    print(\"Sentence's tokens is obtained!\")\n",
        "    totalWords = list(set(totalWords))\n",
        "    dictionary = corpora.Dictionary(sentence_tokens)\n",
        "    dictionary.save( str(model_name) + '.dict')\n",
        "    corpus = [dictionary.doc2bow(text) for text in sentence_tokens]\n",
        "    gensim.corpora.MmCorpus.serialize(str(model_name) + '.mm', corpus)\n",
        "    print(\"Corpus&Dictionary is obtained!\")\n",
        "    print(\"Total unique word count:\" + str(len(totalWords)))\n",
        "\n",
        "    return corpus, dictionary"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4HEHrf9qWTMV"
      },
      "source": [
        "Creating corpus and dictionary for readed file:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V0_OdUs7T0Yt",
        "outputId": "a4eeb79e-21f1-4a8e-f44a-204de539faff"
      },
      "source": [
        "corpus, dictionary = englishSentence(doc_set)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence's tokens is obtained!\n",
            "Corpus&Dictionary is obtained!\n",
            "Total unique word count:2878\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ewae5fXFWbZl"
      },
      "source": [
        "Classic LDA model is obtained and shown: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qT44ZWWBTgr0"
      },
      "source": [
        "classic_LDA = LdaModel(corpus, num_topics = 10, id2word = dictionary, iterations = 100, passes = 10, alpha = 'asymmetric')\n",
        "classic_LDA.save(str(model_name) + '.model')"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s34zSvA2Vix-",
        "outputId": "ae4ac867-8773-4a59-dedb-eea0702efd62"
      },
      "source": [
        "classic_LDA.show_topics(num_topics= 10, num_words=15)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0,\n",
              "  '0.029*\"risk\" + 0.020*\"detect\" + 0.016*\"appl\" + 0.016*\"true\" + 0.015*\"io\" + 0.014*\"friend\" + 0.013*\"final\" + 0.012*\"new\" + 0.012*\"lindsay\" + 0.010*\"updat\" + 0.010*\"microsoft\" + 0.010*\"lohan\" + 0.008*\"releas\" + 0.007*\"sxsw\" + 0.007*\"similar\"'),\n",
              " (1,\n",
              "  '0.061*\"predict\" + 0.022*\"possibl\" + 0.021*\"link\" + 0.020*\"mental\" + 0.015*\"research\" + 0.013*\"babi\" + 0.012*\"facebook\" + 0.011*\"studi\" + 0.011*\"rise\" + 0.011*\"date\" + 0.010*\"read\" + 0.009*\"cut\" + 0.009*\"lena\" + 0.009*\"gomez\" + 0.008*\"dunham\"'),\n",
              " (2,\n",
              "  '0.042*\"bachelor\" + 0.036*\"juan\" + 0.035*\"pablo\" + 0.024*\"care\" + 0.024*\"googl\" + 0.023*\"wearabl\" + 0.019*\"sign\" + 0.018*\"senat\" + 0.017*\"final\" + 0.017*\"android\" + 0.015*\"recal\" + 0.015*\"awar\" + 0.013*\"sdk\" + 0.012*\"galavi\" + 0.012*\"two\"'),\n",
              " (3,\n",
              "  '0.040*\"boy\" + 0.032*\"give\" + 0.029*\"approv\" + 0.026*\"new\" + 0.023*\"show\" + 0.021*\"studi\" + 0.017*\"noah\" + 0.016*\"rais\" + 0.016*\"malefic\" + 0.014*\"miss\" + 0.014*\"hear\" + 0.013*\"trailer\" + 0.013*\"promis\" + 0.013*\"sex\" + 0.012*\"joli\"'),\n",
              " (4,\n",
              "  '0.040*\"one\" + 0.031*\"xbox\" + 0.030*\"earli\" + 0.024*\"titanfal\" + 0.022*\"screen\" + 0.018*\"launch\" + 0.017*\"cancer\" + 0.016*\"access\" + 0.016*\"month\" + 0.016*\"sale\" + 0.016*\"report\" + 0.014*\"colorect\" + 0.012*\"japan\" + 0.012*\"chromecast\" + 0.011*\"day\"'),\n",
              " (5,\n",
              "  '0.046*\"drug\" + 0.044*\"year\" + 0.044*\"studi\" + 0.034*\"compani\" + 0.028*\"research\" + 0.026*\"help\" + 0.018*\"die\" + 0.016*\"confirm\" + 0.015*\"experiment\" + 0.011*\"women\" + 0.010*\"facebook\" + 0.010*\"thousand\" + 0.009*\"user\" + 0.009*\"propos\" + 0.009*\"medic\"'),\n",
              " (6,\n",
              "  '0.046*\"cancer\" + 0.036*\"doctor\" + 0.031*\"oz\" + 0.023*\"cell\" + 0.021*\"weight\" + 0.020*\"ovarian\" + 0.019*\"loss\" + 0.019*\"stem\" + 0.018*\"call\" + 0.017*\"mer\" + 0.016*\"case\" + 0.015*\"saudi\" + 0.014*\"colon\" + 0.014*\"health\" + 0.013*\"retract\"'),\n",
              " (7,\n",
              "  '0.081*\"test\" + 0.075*\"alzheim\" + 0.066*\"blood\" + 0.033*\"drug\" + 0.030*\"us\" + 0.027*\"diseas\" + 0.022*\"new\" + 0.021*\"migrain\" + 0.019*\"fda\" + 0.019*\"save\" + 0.018*\"will\" + 0.018*\"first\" + 0.018*\"prevent\" + 0.017*\"josh\" + 0.016*\"hardi\"'),\n",
              " (8,\n",
              "  '0.053*\"may\" + 0.048*\"health\" + 0.043*\"use\" + 0.035*\"ebola\" + 0.025*\"heart\" + 0.023*\"diabet\" + 0.021*\"cocain\" + 0.021*\"leon\" + 0.021*\"sierra\" + 0.021*\"marijuana\" + 0.019*\"diseas\" + 0.018*\"women\" + 0.018*\"goal\" + 0.017*\"gestat\" + 0.013*\"africa\"'),\n",
              " (9,\n",
              "  '0.074*\"hiv\" + 0.045*\"hospit\" + 0.039*\"old\" + 0.038*\"patient\" + 0.027*\"gel\" + 0.024*\"new\" + 0.024*\"life\" + 0.022*\"warn\" + 0.019*\"hepat\" + 0.017*\"protect\" + 0.015*\"get\" + 0.015*\"exposur\" + 0.014*\"vagin\" + 0.013*\"find\" + 0.012*\"marijuana\"')]"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J7y_82vIQLK8"
      },
      "source": [
        "**n-stage LDA method**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G7_smtkfWmuZ"
      },
      "source": [
        "n-stage LDA method is created:\n",
        "*   model_name: your classic LDA model name\n",
        "*   topic_no: your selected topic number\n",
        "*   word_num: The word count to be selected from the topics for the lda.show_topics() operation\n",
        "*   stage_no: Number of n-stage LDA stages to run, n value (n value starts from 2)\n",
        "*   sentenceTokens: Sentence tokens list for your classic LDA (this code:sentence_tokens in englishSentence function)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TsDFJNE4QQqH"
      },
      "source": [
        "def n_stage_LDA(model_name, topic_no, stage_no, word_num, sentencesTokens):\n",
        "\t#n-stage processes\n",
        "\tfor stage in range(2, stage_no+1):\n",
        "\t\t#Load your LDA model\n",
        "\t\tlda = gensim.models.LdaModel.load(str(model_name) + '.model')\n",
        "\t\tinfo_LDA = lda.show_topics(num_topics= topic_no, num_words=word_num)\n",
        "\t\ttopics = str(info_LDA).split('\\'), (')\n",
        "\t\ttopics[0] = str(topics[0])[2:]\n",
        "\n",
        "\t\twords = []\n",
        "\t\tweights = []\n",
        "\t\ttotalWords = []\n",
        "\t\tfor topic in topics:\n",
        "\t\t\ti = str(topic).find(',')\n",
        "\t\t\ttopicNo = topic[:i]\n",
        "\t\t\twords_weights = str(str(topic[i+3:]).strip()).split('+')\n",
        "\t\t\ttotal_weight = 0\n",
        "\t\t\tfor wordInfo in words_weights: #Determining words and values for each topic\n",
        "\t\t\t\twordInfo = str(wordInfo).strip()\n",
        "\t\t\t\ti = str(wordInfo).find('*')\n",
        "\t\t\t\tweightValue = wordInfo[:i]\n",
        "\t\t\t\tk = str(wordInfo).rfind('\"')\n",
        "\t\t\t\twordValue = wordInfo[i+2:k]\n",
        "\t\t\t\tif weightValue != \"0.0001\": #Evaluation of words with a weight above 0.0001 or your value\n",
        "\t\t\t\t\ttotal_weight += float(weightValue) \n",
        "\t\t\t\t\tweights.append(weightValue) #Calculation total weight for each topic\n",
        "\t\t\t\t\twords.append(wordValue)\n",
        "\t\t\t\telse:\n",
        "\t\t\t\t\tbreak\n",
        "\n",
        "\t\t\ttWords = words #storage words list\n",
        "\t\t\taverage_weight = total_weight/(len(weights)) #Calculation threshold weight value for each topic seperately\n",
        "\t\t\twordCount = 0\n",
        "\t\t\t#Create a new word list based on average weight by looking at the weight of each word\n",
        "\t\t\tfor weight in weights:\n",
        "\t\t\t\tif float(weight) >= average_weight:\n",
        "\t\t\t\t\ttotalWords.append(tWords[wordCount])\n",
        "\t\t\t\t\twordCount += 1\n",
        "\t\t\t\telse:\n",
        "\t\t\t\t\tbreak\n",
        "\t\t\t#Removing contents in words and weights lists for other topic\n",
        "\t\t\twords.clear()\n",
        "\t\t\tweights.clear()\n",
        "\n",
        "\t\ttotalWords = list(set(totalWords)) #Obtaining unique total words \n",
        "\t\tprint(\"New word list is created!\")\n",
        "\n",
        "\t\t#Updating previous sentence's tokens according to total words list and creating new list for this\n",
        "\t\tsentences = sentencesTokens\n",
        "\t\titerative_texts = []\n",
        "\t\tfor text in sentences:\n",
        "\t\t\ttokens = [i for i in text if i in totalWords]\n",
        "\t\t\titerative_texts.append(tokens)\n",
        "\n",
        "\t\tprint(\"New word vocabulary is created! New count: \" + str(len(totalWords)))\n",
        "\n",
        "\t\t#Creating a new LDA model stage\n",
        "\t\tmodel_name = model_name + str(stage) + '-LDA'\n",
        "\t\tdictionary = corpora.Dictionary(iterative_texts)\n",
        "\t\tdictionary.save( model_name + '.dict')\n",
        "\t\tcorpus = [dictionary.doc2bow(iter) for iter in iterative_texts]\n",
        "\t\tgensim.corpora.MmCorpus.serialize(model_name + '.mm', corpus)\n",
        "\t\tprint(\"New corpus&dictionary is created!\")\n",
        "\t\t\n",
        "\t\tstart = time.time()\n",
        "\t\tldamodel = LdaModel(corpus, num_topics = topic_no, id2word = dictionary, iterations = 100, passes = 10, alpha = 'asymmetric')\n",
        "\t\tldamodel.save(str(model_name) + '.model')\n",
        "\t\tend = time.time()\n",
        "\t\tprint(str(stage) + '-LDA model ise created! RunTime:' + str(end-start))"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y2msXSLxYAKP"
      },
      "source": [
        "Your n-stage LDA model is created:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MHDpEHKUUNam",
        "outputId": "3bf1f4cc-601f-4d16-884c-3c4d7a44ac10"
      },
      "source": [
        "n_stage_LDA(model_name, 10, 2, 400, sentence_tokens)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "New word list is created!\n",
            "New word vocabulary is created! New count: 637\n",
            "New corpus&dictionary is created!\n",
            "2-LDA model ise created! RunTime:7.225432872772217\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zteDtPx-YIeA"
      },
      "source": [
        "Your n-stage LDA model is shown:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qv_cttf2Q0B6",
        "outputId": "f08ede00-2e52-4546-d92b-8c8bfb7b8908"
      },
      "source": [
        "n_lda = gensim.models.LdaModel.load(str(model_name) + '2-LDA.model')\n",
        "n_lda.show_topics(num_topics= 10, num_words=15)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0,\n",
              "  '0.029*\"googl\" + 0.023*\"friend\" + 0.022*\"studi\" + 0.022*\"wearabl\" + 0.019*\"lindsay\" + 0.018*\"android\" + 0.017*\"sxsw\" + 0.016*\"approv\" + 0.016*\"senat\" + 0.015*\"lohan\" + 0.014*\"facebook\" + 0.014*\"call\" + 0.012*\"two\" + 0.012*\"sdk\" + 0.012*\"zac\"'),\n",
              " (1,\n",
              "  '0.064*\"detect\" + 0.051*\"true\" + 0.042*\"show\" + 0.031*\"final\" + 0.030*\"link\" + 0.020*\"new\" + 0.019*\"fight\" + 0.018*\"go\" + 0.018*\"amazon\" + 0.018*\"malefic\" + 0.018*\"promis\" + 0.017*\"servic\" + 0.017*\"hbo\" + 0.017*\"season\" + 0.017*\"propos\"'),\n",
              " (2,\n",
              "  '0.126*\"risk\" + 0.059*\"care\" + 0.031*\"similar\" + 0.029*\"noah\" + 0.029*\"tv\" + 0.023*\"support\" + 0.022*\"peopl\" + 0.019*\"beard\" + 0.016*\"miley\" + 0.016*\"lena\" + 0.016*\"jame\" + 0.016*\"dunham\" + 0.015*\"swift\" + 0.015*\"cyru\" + 0.015*\"finalist\"'),\n",
              " (3,\n",
              "  '0.107*\"test\" + 0.049*\"may\" + 0.040*\"diseas\" + 0.039*\"blood\" + 0.036*\"boy\" + 0.034*\"new\" + 0.033*\"alzheim\" + 0.033*\"give\" + 0.027*\"year\" + 0.026*\"life\" + 0.025*\"mer\" + 0.024*\"possibl\" + 0.024*\"health\" + 0.023*\"heart\" + 0.023*\"sign\"'),\n",
              " (4,\n",
              "  '0.058*\"use\" + 0.055*\"research\" + 0.039*\"microsoft\" + 0.036*\"health\" + 0.031*\"cut\" + 0.030*\"mental\" + 0.028*\"case\" + 0.028*\"awar\" + 0.026*\"cocain\" + 0.024*\"new\" + 0.023*\"us\" + 0.023*\"marijuana\" + 0.020*\"cigarett\" + 0.019*\"month\" + 0.019*\"rais\"'),\n",
              " (5,\n",
              "  '0.089*\"alzheim\" + 0.088*\"predict\" + 0.069*\"blood\" + 0.052*\"studi\" + 0.051*\"cancer\" + 0.036*\"ovarian\" + 0.032*\"help\" + 0.029*\"can\" + 0.027*\"chang\" + 0.026*\"say\" + 0.023*\"label\" + 0.021*\"obes\" + 0.021*\"increas\" + 0.020*\"may\" + 0.018*\"miss\"'),\n",
              " (6,\n",
              "  '0.069*\"drug\" + 0.038*\"compani\" + 0.034*\"doctor\" + 0.032*\"oz\" + 0.030*\"cell\" + 0.029*\"medic\" + 0.025*\"stem\" + 0.022*\"weight\" + 0.022*\"fda\" + 0.022*\"migrain\" + 0.021*\"die\" + 0.021*\"hospit\" + 0.020*\"loss\" + 0.020*\"old\" + 0.019*\"save\"'),\n",
              " (7,\n",
              "  '0.079*\"io\" + 0.076*\"appl\" + 0.057*\"titanfal\" + 0.057*\"updat\" + 0.054*\"releas\" + 0.039*\"iphon\" + 0.032*\"samsung\" + 0.031*\"galaxi\" + 0.031*\"new\" + 0.027*\"carplay\" + 0.026*\"one\" + 0.020*\"now\" + 0.018*\"xbox\" + 0.017*\"featur\" + 0.017*\"air\"'),\n",
              " (8,\n",
              "  '0.112*\"cancer\" + 0.065*\"hiv\" + 0.049*\"women\" + 0.046*\"colon\" + 0.039*\"gel\" + 0.038*\"ebola\" + 0.036*\"medicaid\" + 0.036*\"sierra\" + 0.036*\"leon\" + 0.035*\"smoke\" + 0.034*\"colorect\" + 0.033*\"prevent\" + 0.030*\"screen\" + 0.025*\"day\" + 0.022*\"death\"'),\n",
              " (9,\n",
              "  '0.129*\"bachelor\" + 0.105*\"juan\" + 0.105*\"pablo\" + 0.073*\"final\" + 0.036*\"galavi\" + 0.032*\"hi\" + 0.031*\"famili\" + 0.030*\"spoiler\" + 0.027*\"nikki\" + 0.022*\"choos\" + 0.022*\"nation\" + 0.021*\"march\" + 0.020*\"clare\" + 0.019*\"recap\" + 0.019*\"season\"')]"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    }
  ]
}